import numpy as np
from nltk.tokenize import word_tokenize


# Produces an X and y from the derive data, corresponding to bag-of-word counts and the correct label of derive in each file

# Iterate through each subfolder of data/derive

# Iterate through each file in each subfolder

# Iterate through each line

# Parse all potential words, ignoring those uses for the labelling

# NEVERMIND USE THIS SORTA STUFF LOL https://www.linkedin.com/pulse/text-classification-using-bag-words-approach-nltk-scikit-rajendran/

